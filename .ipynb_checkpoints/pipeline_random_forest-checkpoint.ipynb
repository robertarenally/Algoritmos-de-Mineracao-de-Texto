{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - Importanto as bibliotecas\n",
    "import sys #responsável por pegar o diretório por linha de comando\n",
    "import re  \n",
    "import nltk  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets import load_files \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo as condições dos casos de testes\n",
    "steps = [\n",
    "        ('tfidf', TfidfVectorizer(min_df=2)),\n",
    "        ('clf', RandomForestClassifier())\n",
    "]\n",
    "pipe = Pipeline(steps=steps)\n",
    "params = {\n",
    "        'clf__n_estimators':[10,20,30,40,50,60,70,80,90,100]\n",
    "}\n",
    "scoring = ['accuracy','f1_micro', 'f1_macro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando a base de dados\n",
    "argumento = sys.argv[1:]\n",
    "dir = 'r'+str(argumento)\n",
    "dataset = load_files(dir,encoding=\"ISO-8859-1\")\n",
    "x, y = dataset.data, dataset.target \n",
    "\n",
    "#Pré-processamento do texto\n",
    "documents = []\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(x)):  \n",
    "    # Removendo todos os caracteres especiais\n",
    "    document = re.sub(r'\\W', ' ', str(x[sen]))\n",
    "\n",
    "    # removendo todos os caracteres isolados\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Removendo caracter isolado do ínicio\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituindo multiplos espaços por um único espaço\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Convertendo todas as palavras do documento para lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)\n",
    "\n",
    "clf = GridSearchCV(pipe, params, cv=10, n_jobs=-1, verbose=True,scoring=scoring,refit='accuracy',return_train_score=True)\n",
    "clf.fit(documents,y)\n",
    "df = pd.DataFrame(clf.cv_results_)\n",
    "nome_arquivo = 'resultado_random_forest_'+str(argumento)+'.csv'\n",
    "df.to_csv(nome_arquivo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
